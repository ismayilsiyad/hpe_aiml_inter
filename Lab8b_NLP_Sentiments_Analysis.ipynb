{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c0cc588f",
      "metadata": {
        "id": "c0cc588f"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "Sentiment analysis is also known as opinion mining. Sentiment analysis is a type of text mining that finds and extracts subjective information from source material, assisting businesses in determining the social sentiment associated with their brand, product, or service while monitoring online discussions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "91df8baa",
      "metadata": {
        "id": "91df8baa",
        "outputId": "58927452-94f9-4425-b851-ab941977fb6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 32 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 49.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=956514581b98ae0012535f854e697d04f9d58f930fe4f196bc3fcb98b074ee6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.7/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#installing dependency pyspark and findspark\n",
        "#use the below 2 lines only if the packages are not installed\n",
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "\n",
        "#importing the package pyspark and findspark\n",
        "import pyspark\n",
        "import findspark\n",
        "\n",
        "#initializing findspark to set the environment for pyspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "614a2e21",
      "metadata": {
        "id": "614a2e21"
      },
      "source": [
        "Starting the Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cbc907c1",
      "metadata": {
        "id": "cbc907c1"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#The entry point to programming Spark with the Dataset and DataFrame API. \n",
        "spark = SparkSession.builder.appName(\"NLPSAA\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f0151f",
      "metadata": {
        "id": "b4f0151f"
      },
      "source": [
        "Import Important modules required "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a971d950",
      "metadata": {
        "id": "a971d950"
      },
      "outputs": [],
      "source": [
        "#importing pyspark ml sql features\n",
        "from pyspark.ml import Pipeline \n",
        "from pyspark.ml.feature import CountVectorizer,StringIndexer, RegexTokenizer,StopWordsRemover\n",
        "from pyspark.sql.functions import col, udf,regexp_replace,isnull\n",
        "from pyspark.sql.types import StringType,IntegerType\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0ae8f05",
      "metadata": {
        "id": "a0ae8f05"
      },
      "source": [
        "Now we are loading the dataset. The dataset used here contains the tweets with the sentiment value. The 0 represent negative sentiments and 1 represent positive sentiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ec274d",
      "metadata": {
        "id": "25ec274d",
        "outputId": "27789d30-a43b-4f58-8bda-5d46ce75748f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-05802d04bc1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnews_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training.1600000.processed.noemoticon.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnews_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnews_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnews_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnews_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/training.1600000.processed.noemoticon.csv"
          ]
        }
      ],
      "source": [
        "#read the csv containing twitter data\n",
        "news_data = spark.read.csv('training.1600000.processed.noemoticon.csv',header= False)\n",
        "#printing the data\n",
        "news_data.printSchema()\n",
        "news_data.show()\n",
        "news_data = news_data.limit(500)\n",
        "news_data.cache"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a520a22",
      "metadata": {
        "id": "2a520a22"
      },
      "source": [
        "We can check the count of totalitems in the dataset for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca4fcc97",
      "metadata": {
        "id": "ca4fcc97",
        "outputId": "97d5acd1-c2d0-43a7-f432-836814b25368"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#count the rows of dataset\n",
        "news_data.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00df7f9c",
      "metadata": {
        "id": "00df7f9c"
      },
      "source": [
        "We are selecting the titles of tweets and the corresponding category of each tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bab7451",
      "metadata": {
        "id": "7bab7451",
        "outputId": "f5200575-50c3-42bc-ac1a-cc428074ba04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---+\n",
            "|                 _c5|_c0|\n",
            "+--------------------+---+\n",
            "|@switchfoot http:...|  0|\n",
            "|is upset that he ...|  0|\n",
            "|@Kenichan I dived...|  0|\n",
            "|my whole body fee...|  0|\n",
            "|@nationwideclass ...|  0|\n",
            "|@Kwesidei not the...|  0|\n",
            "|         Need a hug |  0|\n",
            "|@LOLTrish hey  lo...|  0|\n",
            "|@Tatiana_K nope t...|  0|\n",
            "|@twittera que me ...|  0|\n",
            "|spring break in p...|  0|\n",
            "|I just re-pierced...|  0|\n",
            "|@caregiving I cou...|  0|\n",
            "|@octolinz16 It it...|  0|\n",
            "|@smarrison i woul...|  0|\n",
            "|@iamjazzyfizzle I...|  0|\n",
            "|Hollis' death sce...|  0|\n",
            "|about to file taxes |  0|\n",
            "|@LettyA ahh ive a...|  0|\n",
            "|@FakerPattyPattz ...|  0|\n",
            "+--------------------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#pick only _c5 and _c0 columns and place in title_category\n",
        "title_category = news_data.select(\"_c5\",\"_c0\")\n",
        "title_category.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfd72e0d",
      "metadata": {
        "id": "cfd72e0d"
      },
      "source": [
        "This is the custom function definition to count the null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "748d0cef",
      "metadata": {
        "id": "748d0cef"
      },
      "outputs": [],
      "source": [
        "#function to count null values in the columns\n",
        "def null_value_count(df):\n",
        "  null_columns_counts = [] #initialize array to null\n",
        "  numRows = df.count() #count the number of rows\n",
        "  for k in df.columns:\n",
        "    nullRows = df.where(col(k).isNull()).count() #count null rows\n",
        "    if(nullRows > 0):\n",
        "      temp = k,nullRows\n",
        "      null_columns_counts.append(temp)\n",
        "  return(null_columns_counts) #return count of null collumns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeaa0a96",
      "metadata": {
        "id": "aeaa0a96"
      },
      "source": [
        "We are applying the custom function to the data frsme title_category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b9d67a",
      "metadata": {
        "id": "11b9d67a"
      },
      "outputs": [],
      "source": [
        "null_columns_count_list = null_value_count(title_category)\n",
        "#spark.createDataFrame(null_columns_count_list, ['Column_With_Null_Value', 'Null_Values_Count']).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b1840e9",
      "metadata": {
        "id": "3b1840e9"
      },
      "source": [
        "# Cleaning the dataset\n",
        "\n",
        "Now we can drop the null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "534b2a42",
      "metadata": {
        "id": "534b2a42",
        "outputId": "e2b9fef4-4103-4680-dc9a-43d658740cb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------+---+\n",
            "|_c5                                                                                                                  |_c0|\n",
            "+---------------------------------------------------------------------------------------------------------------------+---+\n",
            "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  |0  |\n",
            "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |0  |\n",
            "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            |0  |\n",
            "|my whole body feels itchy and like its on fire                                                                       |0  |\n",
            "|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |0  |\n",
            "|@Kwesidei not the whole crew                                                                                         |0  |\n",
            "|Need a hug                                                                                                           |0  |\n",
            "|@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |0  |\n",
            "|@Tatiana_K nope they didn't have it                                                                                  |0  |\n",
            "|@twittera que me muera ?                                                                                             |0  |\n",
            "|spring break in plain city... it's snowing                                                                           |0  |\n",
            "|I just re-pierced my ears                                                                                            |0  |\n",
            "|@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |0  |\n",
            "|@octolinz16 It it counts, idk why I did either. you never talk to me anymore                                         |0  |\n",
            "|@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|0  |\n",
            "|@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |0  |\n",
            "|Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |0  |\n",
            "|about to file taxes                                                                                                  |0  |\n",
            "|@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |0  |\n",
            "|@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |0  |\n",
            "+---------------------------------------------------------------------------------------------------------------------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#drop not applicable and null values from the category\n",
        "title_category = title_category.dropna()\n",
        "title_category.count()\n",
        "title_category.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86548d4",
      "metadata": {
        "id": "c86548d4",
        "outputId": "2e945b46-fe60-443c-a180-e647f58ca78d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Tweets: string (nullable = true)\n",
            " |-- Sentiment: string (nullable = true)\n",
            "\n",
            "+--------------------+---------+\n",
            "|              Tweets|Sentiment|\n",
            "+--------------------+---------+\n",
            "|@switchfoot http:...|        0|\n",
            "|is upset that he ...|        0|\n",
            "|@Kenichan I dived...|        0|\n",
            "|my whole body fee...|        0|\n",
            "|@nationwideclass ...|        0|\n",
            "|@Kwesidei not the...|        0|\n",
            "|         Need a hug |        0|\n",
            "|@LOLTrish hey  lo...|        0|\n",
            "|@Tatiana_K nope t...|        0|\n",
            "|@twittera que me ...|        0|\n",
            "|spring break in p...|        0|\n",
            "|I just re-pierced...|        0|\n",
            "|@caregiving I cou...|        0|\n",
            "|@octolinz16 It it...|        0|\n",
            "|@smarrison i woul...|        0|\n",
            "|@iamjazzyfizzle I...|        0|\n",
            "|Hollis' death sce...|        0|\n",
            "|about to file taxes |        0|\n",
            "|@LettyA ahh ive a...|        0|\n",
            "|@FakerPattyPattz ...|        0|\n",
            "+--------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from functools import reduce\n",
        "\n",
        "#data containing from the csv\n",
        "oldColumns = title_category.schema.names\n",
        "#creating new columns with heading 'Tweets' and 'Sentiment'\n",
        "newColumns = ['Tweets','Sentiment']\n",
        "\n",
        "#applying lambda function to create a copy of old columns to new columns\n",
        "title_category = reduce(lambda title_category, idx: title_category.withColumnRenamed(oldColumns[idx], newColumns[idx]),range(len(oldColumns)), title_category)\n",
        "title_category.printSchema()\n",
        "title_category.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ad4ef83",
      "metadata": {
        "id": "8ad4ef83"
      },
      "source": [
        "Now we can remove the numbers in tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e67a8cd",
      "metadata": {
        "id": "1e67a8cd",
        "outputId": "2988a0ce-a75e-44ea-e41b-868f04362dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
            "|Tweets                                                                                                               |only_str                                                                                                             |\n",
            "+---------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
            "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  |@switchfoot http://twitpic.com/yzl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D    |\n",
            "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |\n",
            "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            |@Kenichan I dived many times for the ball. Managed to save %  The rest go out of bounds                              |\n",
            "|my whole body feels itchy and like its on fire                                                                       |my whole body feels itchy and like its on fire                                                                       |\n",
            "|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |\n",
            "|@Kwesidei not the whole crew                                                                                         |@Kwesidei not the whole crew                                                                                         |\n",
            "|Need a hug                                                                                                           |Need a hug                                                                                                           |\n",
            "|@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |\n",
            "|@Tatiana_K nope they didn't have it                                                                                  |@Tatiana_K nope they didn't have it                                                                                  |\n",
            "|@twittera que me muera ?                                                                                             |@twittera que me muera ?                                                                                             |\n",
            "|spring break in plain city... it's snowing                                                                           |spring break in plain city... it's snowing                                                                           |\n",
            "|I just re-pierced my ears                                                                                            |I just re-pierced my ears                                                                                            |\n",
            "|@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |\n",
            "|@octolinz16 It it counts, idk why I did either. you never talk to me anymore                                         |@octolinz It it counts, idk why I did either. you never talk to me anymore                                           |\n",
            "|@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|\n",
            "|@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |\n",
            "|Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |\n",
            "|about to file taxes                                                                                                  |about to file taxes                                                                                                  |\n",
            "|@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |\n",
            "|@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |\n",
            "+---------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#cleaning the numbers from tweets\n",
        "title_category = title_category.withColumn(\"only_str\",regexp_replace(col('Tweets'), '\\d+', ''))\n",
        "title_category.select(\"Tweets\",\"only_str\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b49f034f",
      "metadata": {
        "id": "b49f034f"
      },
      "source": [
        "Split the text into constituent words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dae25af",
      "metadata": {
        "id": "1dae25af",
        "outputId": "53236734-9acb-46f6-c0aa-6ece40608478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+--------------------+--------------------+\n",
            "|              Tweets|Sentiment|            only_str|               words|\n",
            "+--------------------+---------+--------------------+--------------------+\n",
            "|@switchfoot http:...|        0|@switchfoot http:...|[switchfoot, http...|\n",
            "|is upset that he ...|        0|is upset that he ...|[is, upset, that,...|\n",
            "|@Kenichan I dived...|        0|@Kenichan I dived...|[kenichan, i, div...|\n",
            "|my whole body fee...|        0|my whole body fee...|[my, whole, body,...|\n",
            "|@nationwideclass ...|        0|@nationwideclass ...|[nationwideclass,...|\n",
            "|@Kwesidei not the...|        0|@Kwesidei not the...|[kwesidei, not, t...|\n",
            "|         Need a hug |        0|         Need a hug |      [need, a, hug]|\n",
            "|@LOLTrish hey  lo...|        0|@LOLTrish hey  lo...|[loltrish, hey, l...|\n",
            "|@Tatiana_K nope t...|        0|@Tatiana_K nope t...|[tatiana_k, nope,...|\n",
            "|@twittera que me ...|        0|@twittera que me ...|[twittera, que, m...|\n",
            "|spring break in p...|        0|spring break in p...|[spring, break, i...|\n",
            "|I just re-pierced...|        0|I just re-pierced...|[i, just, re, pie...|\n",
            "|@caregiving I cou...|        0|@caregiving I cou...|[caregiving, i, c...|\n",
            "|@octolinz16 It it...|        0|@octolinz It it c...|[octolinz, it, it...|\n",
            "|@smarrison i woul...|        0|@smarrison i woul...|[smarrison, i, wo...|\n",
            "|@iamjazzyfizzle I...|        0|@iamjazzyfizzle I...|[iamjazzyfizzle, ...|\n",
            "|Hollis' death sce...|        0|Hollis' death sce...|[hollis, death, s...|\n",
            "|about to file taxes |        0|about to file taxes |[about, to, file,...|\n",
            "|@LettyA ahh ive a...|        0|@LettyA ahh ive a...|[lettya, ahh, ive...|\n",
            "|@FakerPattyPattz ...|        0|@FakerPattyPattz ...|[fakerpattypattz,...|\n",
            "+--------------------+---------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#split the text to words or tokens\n",
        "#https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.RegexTokenizer.html\n",
        "regex_tokenizer = RegexTokenizer(inputCol=\"only_str\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "raw_words = regex_tokenizer.transform(title_category)\n",
        "raw_words.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1526619",
      "metadata": {
        "id": "d1526619"
      },
      "source": [
        "Remove the stop words from segregated list of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d5434a",
      "metadata": {
        "id": "94d5434a",
        "outputId": "fcd3c94d-5199-4091-e677-d8980b7d2431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+\n",
            "|words                                                                                                                                |filtered                                                                                     |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+\n",
            "|[switchfoot, http, twitpic, com, yzl, awww, that, s, a, bummer, you, shoulda, got, david, carr, of, third, day, to, do, it, d]       |[switchfoot, http, twitpic, com, yzl, awww, bummer, shoulda, got, david, carr, third, day, d]|\n",
            "|[is, upset, that, he, can, t, update, his, facebook, by, texting, it, and, might, cry, as, a, result, school, today, also, blah]     |[upset, update, facebook, texting, might, cry, result, school, today, also, blah]            |\n",
            "|[kenichan, i, dived, many, times, for, the, ball, managed, to, save, the, rest, go, out, of, bounds]                                 |[kenichan, dived, many, times, ball, managed, save, rest, go, bounds]                        |\n",
            "|[my, whole, body, feels, itchy, and, like, its, on, fire]                                                                            |[whole, body, feels, itchy, like, fire]                                                      |\n",
            "|[nationwideclass, no, it, s, not, behaving, at, all, i, m, mad, why, am, i, here, because, i, can, t, see, you, all, over, there]    |[nationwideclass, behaving, m, mad, see]                                                     |\n",
            "|[kwesidei, not, the, whole, crew]                                                                                                    |[kwesidei, whole, crew]                                                                      |\n",
            "|[need, a, hug]                                                                                                                       |[need, hug]                                                                                  |\n",
            "|[loltrish, hey, long, time, no, see, yes, rains, a, bit, only, a, bit, lol, i, m, fine, thanks, how, s, you]                         |[loltrish, hey, long, time, see, yes, rains, bit, bit, lol, m, fine, thanks]                 |\n",
            "|[tatiana_k, nope, they, didn, t, have, it]                                                                                           |[tatiana_k, nope, didn]                                                                      |\n",
            "|[twittera, que, me, muera]                                                                                                           |[twittera, que, muera]                                                                       |\n",
            "|[spring, break, in, plain, city, it, s, snowing]                                                                                     |[spring, break, plain, city, snowing]                                                        |\n",
            "|[i, just, re, pierced, my, ears]                                                                                                     |[re, pierced, ears]                                                                          |\n",
            "|[caregiving, i, couldn, t, bear, to, watch, it, and, i, thought, the, ua, loss, was, embarrassing]                                   |[caregiving, couldn, bear, watch, thought, ua, loss, embarrassing]                           |\n",
            "|[octolinz, it, it, counts, idk, why, i, did, either, you, never, talk, to, me, anymore]                                              |[octolinz, counts, idk, either, never, talk, anymore]                                        |\n",
            "|[smarrison, i, would, ve, been, the, first, but, i, didn, t, have, a, gun, not, really, though, zac, snyder, s, just, a, doucheclown]|[smarrison, ve, first, didn, gun, really, though, zac, snyder, doucheclown]                  |\n",
            "|[iamjazzyfizzle, i, wish, i, got, to, watch, it, with, you, i, miss, you, and, iamlilnicki, how, was, the, premiere]                 |[iamjazzyfizzle, wish, got, watch, miss, iamlilnicki, premiere]                              |\n",
            "|[hollis, death, scene, will, hurt, me, severely, to, watch, on, film, wry, is, directors, cut, not, out, now]                        |[hollis, death, scene, hurt, severely, watch, film, wry, directors, cut]                     |\n",
            "|[about, to, file, taxes]                                                                                                             |[file, taxes]                                                                                |\n",
            "|[lettya, ahh, ive, always, wanted, to, see, rent, love, the, soundtrack]                                                             |[lettya, ahh, ive, always, wanted, see, rent, love, soundtrack]                              |\n",
            "|[fakerpattypattz, oh, dear, were, you, drinking, out, of, the, forgotten, table, drinks]                                             |[fakerpattypattz, oh, dear, drinking, forgotten, table, drinks]                              |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Removing the stop words from the list of words\n",
        "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StopWordsRemover.html\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "words_df = remover.transform(raw_words)\n",
        "words_df.select(\"words\",\"filtered\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10e215cd",
      "metadata": {
        "id": "10e215cd"
      },
      "source": [
        "Convert text into vectors of token counts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ff14dc4",
      "metadata": {
        "id": "7ff14dc4"
      },
      "source": [
        "# Partition the dataset into training and test datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1efd33e",
      "metadata": {
        "id": "a1efd33e",
        "outputId": "5c763c58-2bd4-465c-8721-b09bd961d5f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+--------------------+--------------------+--------------------+\n",
            "|              Tweets|Sentiment|            only_str|               words|            filtered|\n",
            "+--------------------+---------+--------------------+--------------------+--------------------+\n",
            "| Body Of Missing ...|        0| Body Of Missing ...|[body, of, missin...|[body, missing, n...|\n",
            "| wonder if Jon lo...|        0| wonder if Jon lo...|[wonder, if, jon,...|[wonder, jon, los...|\n",
            "|#3 woke up and wa...|        0|# woke up and was...|[woke, up, and, w...|[woke, accident, ...|\n",
            "|&quot;On popular ...|        0|&quot;On popular ...|[quot, on, popula...|[quot, popular, m...|\n",
            "|...and, India mis...|        0|...and, India mis...|[and, india, miss...|[india, missed, t...|\n",
            "|@AmaNorris wow th...|        0|@AmaNorris wow th...|[amanorris, wow, ...|[amanorris, wow, ...|\n",
            "|@Appomattox_News ...|        0|@Appomattox_News ...|[appomattox_news,...|[appomattox_news,...|\n",
            "|@B_Barnett I did ...|        0|@B_Barnett I did ...|[b_barnett, i, di...|[b_barnett, reall...|\n",
            "|@BatManYNG I miss...|        0|@BatManYNG I miss...|[batmanyng, i, mi...|[batmanyng, miss,...|\n",
            "|@Brandizzzle08 yo...|        0|@Brandizzzle yoyo...|[brandizzzle, yoy...|[brandizzzle, yoy...|\n",
            "|@CarVin1 lol they...|        0|@CarVin lol they ...|[carvin, lol, the...|[carvin, lol, emo...|\n",
            "|@David_Henrie *th...|        0|@David_Henrie *th...|[david_henrie, th...|[david_henrie, th...|\n",
            "|@DiannePulham OOO...|        0|@DiannePulham OOO...|[diannepulham, oo...|[diannepulham, oo...|\n",
            "|@DjAlizay I reall...|        0|@DjAlizay I reall...|[djalizay, i, rea...|[djalizay, really...|\n",
            "|@DonnieWahlberg o...|        0|@DonnieWahlberg o...|[donniewahlberg, ...|[donniewahlberg, ...|\n",
            "|@EazyDoesIt87 NEG...|        0|@EazyDoesIt NEGAT...|[eazydoesit, nega...|[eazydoesit, nega...|\n",
            "|@FakerPattyPattz ...|        0|@FakerPattyPattz ...|[fakerpattypattz,...|[fakerpattypattz,...|\n",
            "|@FranzGlaus I kno...|        0|@FranzGlaus I kno...|[franzglaus, i, k...|[franzglaus, know...|\n",
            "|@GuruMN but this ...|        0|@GuruMN but this ...|[gurumn, but, thi...|[gurumn, canada, ...|\n",
            "|@Henkuyinepu yeah...|        0|@Henkuyinepu yeah...|[henkuyinepu, yea...|[henkuyinepu, yea...|\n",
            "+--------------------+---------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------+---------+--------------------+--------------------+--------------------+\n",
            "|              Tweets|Sentiment|            only_str|               words|            filtered|\n",
            "+--------------------+---------+--------------------+--------------------+--------------------+\n",
            "|..and of course.....|        0|..and of course.....|[and, of, course,...|[course, access, ...|\n",
            "|@Alliana07 it did...|        0|@Alliana it didn'...|[alliana, it, did...|[alliana, didn, m...|\n",
            "|@BridgetsBeaches ...|        0|@BridgetsBeaches ...|[bridgetsbeaches,...|[bridgetsbeaches,...|\n",
            "|@Brodhe geez ur n...|        0|@Brodhe geez ur n...|[brodhe, geez, ur...|[brodhe, geez, ur...|\n",
            "|@CaitlinOConnor i...|        0|@CaitlinOConnor i...|[caitlinoconnor, ...|[caitlinoconnor, ...|\n",
            "|@ColinDeMar Far t...|        0|@ColinDeMar Far t...|[colindemar, far,...|[colindemar, far,...|\n",
            "|@Dangerm0use I th...|        0|@Dangermuse I thi...|[dangermuse, i, t...|[dangermuse, thin...|\n",
            "|@DonnieWahlberg I...|        0|@DonnieWahlberg I...|[donniewahlberg, ...|[donniewahlberg, ...|\n",
            "|@Henkuyinepu it's...|        0|@Henkuyinepu it's...|[henkuyinepu, it,...|[henkuyinepu, ove...|\n",
            "|@HibaNick yeah aw...|        0|@HibaNick yeah aw...|[hibanick, yeah, ...|[hibanick, yeah, ...|\n",
            "|@HumpNinja I cry ...|        0|@HumpNinja I cry ...|[humpninja, i, cr...|[humpninja, cry, ...|\n",
            "|@JonathanRKnight ...|        0|@JonathanRKnight ...|[jonathanrknight,...|[jonathanrknight,...|\n",
            "|@Kenichan I dived...|        0|@Kenichan I dived...|[kenichan, i, div...|[kenichan, dived,...|\n",
            "|@LevenRambin: Tak...|        0|@LevenRambin: Tak...|[levenrambin, tak...|[levenrambin, tak...|\n",
            "|@MissXu sorry! be...|        0|@MissXu sorry! be...|[missxu, sorry, b...|[missxu, sorry, b...|\n",
            "|@Starrbby too bad...|        0|@Starrbby too bad...|[starrbby, too, b...|[starrbby, bad, w...|\n",
            "|@ThaStevieG but w...|        0|@ThaStevieG but w...|[thastevieg, but,...|[thastevieg, real...|\n",
            "|@aaronrva is in t...|        0|@aaronrva is in t...|[aaronrva, is, in...|[aaronrva, bathro...|\n",
            "|@alicayaba so cuu...|        0|@alicayaba so cuu...|[alicayaba, so, c...|[alicayaba, cuuut...|\n",
            "|@austinhill I wis...|        0|@austinhill I wis...|[austinhill, i, w...|[austinhill, wish...|\n",
            "+--------------------+---------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Partition the dataset into trainingData 80% and testData 20%\n",
        "(trainingData, testData) = words_df.randomSplit([0.8, 0.2],seed = 11)\n",
        "trainingData.show()\n",
        "testData.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1df21afb",
      "metadata": {
        "id": "1df21afb"
      },
      "source": [
        "# Model Training and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0267532",
      "metadata": {
        "id": "e0267532",
        "outputId": "82a52fc3-bfe3-4996-c5af-acd8922e2d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|              Tweets|Sentiment|            only_str|               words|            filtered|                  tf|            features|\n",
            "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "| Body Of Missing ...|        0| Body Of Missing ...|[body, of, missin...|[body, missing, n...|(65536,[731,3159,...|(65536,[731,3159,...|\n",
            "| wonder if Jon lo...|        0| wonder if Jon lo...|[wonder, if, jon,...|[wonder, jon, los...|(65536,[19153,329...|(65536,[19153,329...|\n",
            "|#3 woke up and wa...|        0|# woke up and was...|[woke, up, and, w...|[woke, accident, ...|(65536,[5660,7427...|(65536,[5660,7427...|\n",
            "|&quot;On popular ...|        0|&quot;On popular ...|[quot, on, popula...|[quot, popular, m...|(65536,[178,1903,...|(65536,[178,1903,...|\n",
            "|...and, India mis...|        0|...and, India mis...|[and, india, miss...|[india, missed, t...|(65536,[12350,228...|(65536,[12350,228...|\n",
            "+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "#tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "#HashingTF maps a sequence of terms\n",
        "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.HashingTF.html\n",
        "hashtf = HashingTF(numFeatures=2**16, inputCol=\"filtered\", outputCol='tf')\n",
        "\n",
        "#IDF stands for Inverse Document Frequency (Common:Rare)\n",
        "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.feature.IDF.html\n",
        "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "#label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
        "pipeline = Pipeline(stages=[hashtf, idf])\n",
        "\n",
        "pipelineFit = pipeline.fit(trainingData)\n",
        "train_df = pipelineFit.transform(trainingData)\n",
        "val_df = pipelineFit.transform(testData)\n",
        "train_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae08819f",
      "metadata": {
        "id": "ae08819f"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "train_df=train_df.withColumnRenamed('Sentiment', 'label')\n",
        "train_df=train_df.withColumn(\"label\",train_df.label.cast('int'))\n",
        "val_df=val_df.withColumnRenamed('Sentiment', 'label')\n",
        "val_df=val_df.withColumn(\"label\",val_df.label.cast('int'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e6a7f6b",
      "metadata": {
        "id": "3e6a7f6b"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80efef2b",
      "metadata": {
        "id": "80efef2b"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "## Fitting the model\n",
        "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
        "lrModel = lr.fit(train_df)\n",
        "lrPreds = lrModel.transform(val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "770aa79f",
      "metadata": {
        "id": "770aa79f",
        "outputId": "20fe4ca2-eb80-4254-daa8-4477ac0b3c2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Logistic Regression is = 1\n"
          ]
        }
      ],
      "source": [
        "## Evaluating the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "lr_accuracy = evaluator.evaluate(lrPreds)\n",
        "print(\"Accuracy of Logistic Regression is = %g\"% (lr_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "900ccda5",
      "metadata": {
        "id": "900ccda5"
      },
      "source": [
        "## Decision Tree Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6523a330",
      "metadata": {
        "id": "6523a330"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "## Fitting the model\n",
        "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.DecisionTreeClassifier.html\n",
        "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
        "dtModel = dt.fit(train_df)\n",
        "dtPreds = dtModel.transform(val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9272dae8",
      "metadata": {
        "id": "9272dae8",
        "outputId": "fe89bdfd-0dbb-431c-d3cc-d8b51760e484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Decision Trees is = 1\n"
          ]
        }
      ],
      "source": [
        "## Evaluating the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "dt_accuracy = evaluator.evaluate(dtPreds)\n",
        "#Accuracy of Decision Tree\n",
        "print(\"Accuracy of Decision Trees is = %g\"% (dt_accuracy))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a8c9ff7",
      "metadata": {
        "id": "0a8c9ff7"
      },
      "source": [
        "# Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b415fcb5",
      "metadata": {
        "id": "b415fcb5"
      },
      "outputs": [],
      "source": [
        "#applying NaiveBays algorithm\n",
        "nb = NaiveBayes(modelType=\"multinomial\",labelCol=\"label\", featuresCol=\"features\")\n",
        "nbModel = nb.fit(train_df)\n",
        "#get the prediction by transforming the model\n",
        "nb_predictions = nbModel.transform(val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "962b810e",
      "metadata": {
        "id": "962b810e",
        "outputId": "0e288f8b-d39f-4d5d-8dc1-c2da3b0d26a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----+--------------------+\n",
            "|prediction|label|            features|\n",
            "+----------+-----+--------------------+\n",
            "|       0.0|    0|(65536,[2464,1430...|\n",
            "|       0.0|    0|(65536,[1903,4529...|\n",
            "|       0.0|    0|(65536,[1903,9859...|\n",
            "|       0.0|    0|(65536,[12806,230...|\n",
            "|       0.0|    0|(65536,[14013,171...|\n",
            "|       0.0|    0|(65536,[22076,344...|\n",
            "|       0.0|    0|(65536,[22351,260...|\n",
            "|       0.0|    0|(65536,[8741,2418...|\n",
            "|       0.0|    0|(65536,[13712,159...|\n",
            "|       0.0|    0|(65536,[2762,6040...|\n",
            "|       0.0|    0|(65536,[5827,8449...|\n",
            "|       0.0|    0|(65536,[7173,3399...|\n",
            "|       0.0|    0|(65536,[2548,2888...|\n",
            "|       0.0|    0|(65536,[31448,478...|\n",
            "|       0.0|    0|(65536,[308,13889...|\n",
            "|       0.0|    0|(65536,[6040,6122...|\n",
            "|       0.0|    0|(65536,[1198,4207...|\n",
            "|       0.0|    0|(65536,[2338,4166...|\n",
            "|       0.0|    0|(65536,[3386,2202...|\n",
            "|       0.0|    0|(65536,[9859,1298...|\n",
            "|       0.0|    0|(65536,[7194,1908...|\n",
            "|       0.0|    0|(65536,[6042,8923...|\n",
            "|       0.0|    0|(65536,[9859,1573...|\n",
            "|       0.0|    0|(65536,[338,4570,...|\n",
            "|       0.0|    0|(65536,[18312,264...|\n",
            "|       0.0|    0|(65536,[9347,1107...|\n",
            "|       0.0|    0|(65536,[6498,8789...|\n",
            "|       0.0|    0|(65536,[8461,8538...|\n",
            "|       0.0|    0|(65536,[389,1198,...|\n",
            "|       0.0|    0|(65536,[18184,218...|\n",
            "|       0.0|    0|(65536,[44279],[0...|\n",
            "|       0.0|    0|(65536,[44279],[0...|\n",
            "|       0.0|    0|(65536,[6052,3468...|\n",
            "|       0.0|    0|(65536,[3656,1448...|\n",
            "|       0.0|    0|(65536,[1968,6366...|\n",
            "|       0.0|    0|(65536,[6964,1818...|\n",
            "|       0.0|    0|(65536,[1602,3053...|\n",
            "|       0.0|    0|(65536,[8079,2417...|\n",
            "|       0.0|    0|(65536,[7173,3906...|\n",
            "|       0.0|    0|(65536,[6964,2635...|\n",
            "|       0.0|    0|(65536,[3861,7194...|\n",
            "|       0.0|    0|(65536,[1198,2549...|\n",
            "|       0.0|    0|(65536,[9012,1079...|\n",
            "|       0.0|    0|(65536,[1797,1123...|\n",
            "|       0.0|    0|(65536,[22105,310...|\n",
            "|       0.0|    0|(65536,[338,3639,...|\n",
            "|       0.0|    0|(65536,[737,13781...|\n",
            "|       0.0|    0|(65536,[29129,368...|\n",
            "|       0.0|    0|(65536,[22137,328...|\n",
            "|       0.0|    0|(65536,[3894,1762...|\n",
            "|       0.0|    0|(65536,[2606,9129...|\n",
            "|       0.0|    0|(65536,[7173,3356...|\n",
            "|       0.0|    0|(65536,[106,17625...|\n",
            "|       0.0|    0|(65536,[518,8031,...|\n",
            "|       0.0|    0|(65536,[12086,188...|\n",
            "|       0.0|    0|(65536,[17603,240...|\n",
            "|       0.0|    0|(65536,[2062,1035...|\n",
            "|       0.0|    0|(65536,[16967,218...|\n",
            "|       0.0|    0|(65536,[1198,1235...|\n",
            "|       0.0|    0|(65536,[1706,3681...|\n",
            "|       0.0|    0|(65536,[32,3053,1...|\n",
            "|       0.0|    0|(65536,[8317,3205...|\n",
            "|       0.0|    0|(65536,[6261,1541...|\n",
            "|       0.0|    0|(65536,[6036,9084...|\n",
            "|       0.0|    0|(65536,[8936,2365...|\n",
            "|       0.0|    0|(65536,[1198,7713...|\n",
            "|       0.0|    0|(65536,[8317,1055...|\n",
            "|       0.0|    0|(65536,[11136,201...|\n",
            "|       0.0|    0|(65536,[11650,163...|\n",
            "|       0.0|    0|(65536,[15148,173...|\n",
            "|       0.0|    0|(65536,[6781,7014...|\n",
            "|       0.0|    0|(65536,[4570,2565...|\n",
            "|       0.0|    0|(65536,[2458,1514...|\n",
            "|       0.0|    0|(65536,[7062,2007...|\n",
            "|       0.0|    0|(65536,[47153,558...|\n",
            "|       0.0|    0|(65536,[2808,5914...|\n",
            "|       0.0|    0|(65536,[24651,291...|\n",
            "|       0.0|    0|(65536,[2973,2142...|\n",
            "|       0.0|    0|(65536,[3861,1165...|\n",
            "|       0.0|    0|(65536,[7823,8408...|\n",
            "|       0.0|    0|(65536,[1602,6905...|\n",
            "|       0.0|    0|(65536,[15139,153...|\n",
            "|       0.0|    0|(65536,[1124,3098...|\n",
            "|       0.0|    0|(65536,[15295,154...|\n",
            "|       0.0|    0|(65536,[7129,8538...|\n",
            "|       0.0|    0|(65536,[3694,3612...|\n",
            "|       0.0|    0|(65536,[2268,2306...|\n",
            "|       0.0|    0|(65536,[1328,4114...|\n",
            "|       0.0|    0|(65536,[6036,1789...|\n",
            "|       0.0|    0|(65536,[2710,1411...|\n",
            "|       0.0|    0|(65536,[55666,588...|\n",
            "|       0.0|    0|(65536,[1198,1589...|\n",
            "|       0.0|    0|(65536,[518,9029,...|\n",
            "|       0.0|    0|(65536,[1589,2892...|\n",
            "|       0.0|    0|(65536,[1257,9704...|\n",
            "|       0.0|    0|(65536,[2408,1079...|\n",
            "|       0.0|    0|(65536,[4628,5879...|\n",
            "|       0.0|    0|(65536,[17893,482...|\n",
            "|       0.0|    0|(65536,[921,1689,...|\n",
            "|       0.0|    0|(65536,[2437,1313...|\n",
            "+----------+-----+--------------------+\n",
            "only showing top 100 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nb_predictions.select(\"prediction\", \"label\", \"features\").show(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1781118c",
      "metadata": {
        "id": "1781118c",
        "outputId": "2abae7e7-7332-42f1-8c20-8e0d9b8f182f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of NaiveBayes is = 1\n",
            "Test Error of NaiveBayes = 0 \n"
          ]
        }
      ],
      "source": [
        "#evaluate and print the accuracy\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "nb_accuracy = evaluator.evaluate(nb_predictions)\n",
        "print(\"Accuracy of NaiveBayes is = %g\"% (nb_accuracy))\n",
        "print(\"Test Error of NaiveBayes = %g \" % (1.0 - nb_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e13d92",
      "metadata": {
        "id": "e7e13d92"
      },
      "outputs": [],
      "source": [
        "#https://www.datatechnotes.com/2021/12/mllib-naive-bayes-classification.html"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Lab1_NLP-Sentiments_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}